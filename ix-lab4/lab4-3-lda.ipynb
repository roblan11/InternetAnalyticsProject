{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 3: Latent Dirichlet allocation\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** K\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* Kim Lan Phan Hoang\n",
    "* Robin Lang\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 3 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "from collections import defaultdict\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "import re\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')\n",
    "commonWords = [\"student\",\"students\",\"learning\",\"course\",\"courses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get RDD of courses\n",
    "data = sc.parallelize(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transform documents into bags of word\n",
    "    #take only the description into account\n",
    "    #split depending on whitespace and punctuation\n",
    "    #only keep letter words\n",
    "    #lowercase words\n",
    "    #remove stopwords\n",
    "    #remove non-relevant words\n",
    "wordsInDocument = data.map(lambda x: x[\"description\"])\\\n",
    ".map(lambda x: re.split(\"[\\s\\.\\?\\!\\,\\;\\:\\-\\(\\)\\[\\]\\{\\}\\\"\\/]\", x))\\\n",
    ".map(lambda w: [x for x in w if x.isalpha()])\\\n",
    ".map(lambda w: [x for x in w if len(x)>3])\\\n",
    ".map(lambda w: [x.lower() for x in w])\\\n",
    ".map(lambda w: [x for x in w if (x not in stopwords) and (x not in commonWords)])\n",
    "\n",
    "\n",
    "#Group words in order to obtain a word list\n",
    "    #add a counter\n",
    "    #reducebykey to group words\n",
    "    #descending sort on number of recurrence\n",
    "mostReccurentWords = wordsInDocument.flatMap(lambda x: x)\\\n",
    ".map(lambda word: (word, 1))\\\n",
    ".reduceByKey(lambda x,y: x + y)\\\n",
    ".map(lambda x: (x[1], x[0]))\\\n",
    ".sortByKey(False)\n",
    "\n",
    "    \n",
    "# get all possible words and map them to an id\n",
    "wordsList = mostReccurentWords.map(lambda x: x[1]).zipWithIndex().collectAsMap()\n",
    "\n",
    "distinctNbWords = len(wordsList)\n",
    "\n",
    "    \n",
    "# get a document and return a tuple (id , vector of word occurrencies )\n",
    "def documentToVector(d):\n",
    "    \n",
    "    wordOccurrencies = defaultdict(int) #initialize dict\n",
    "    for w in d[0]: \n",
    "        wordOccurrencies[wordsList[w]] += 1 # add one at word_id position\n",
    "            \n",
    "    wordOccurrencies = sorted(wordOccurrencies.items()) # in order to obtain a list for each element\n",
    "    wordOcc_0 = [x[0] for x in wordOccurrencies]\n",
    "    wordOcc_1 = [x[1] for x in wordOccurrencies]\n",
    "    \n",
    "    return (d[1], Vectors.sparse(distinctNbWords, wordOcc_0, wordOcc_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.8: Topics extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your pre-processed courses dataset, extract topics using LDA. Print k = 10 topics extracted using LDA and give them labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get right format to submit to lda\n",
    "    # zipWithIndex to obtain an id for the document\n",
    "    # get a vector of used words\n",
    "documents = wordsInDocument.zipWithIndex().map(documentToVector).map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create lda model\n",
    "lda = LDA.train(documents, k = 10)\n",
    "\n",
    "# get topics\n",
    "topic_indices = lda.describeTopics(maxTermsPerTopic = 10) # 10 words to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsList_helper = mostReccurentWords.map(lambda x: x[1]).collect()\n",
    "\n",
    "def displayTopicsWords(lda_topics, alphaValues, betaValues):\n",
    "    for i in range(len(lda_topics)):\n",
    "        print(\"alpha = \",alphaValues[i],\"& beta =\",betaValues[i])\n",
    "        for j in range(10):\n",
    "            print(\"- Topic \",j+1,\": \",end=\"\")\n",
    "            for w in lda_topics[i][j][0]:\n",
    "                print(wordsList_helper[w],\"-\", end=\"\")\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha =  default & beta = default\n",
      "- Topic  1 : methods -models -model -stochastic -theory -time -introduction -exam -financial -risk -\n",
      "- Topic  2 : methods -case -energy -management -content -work -assessment -outcomes -business -evaluate -\n",
      "- Topic  3 : methods -energy -equations -content -processes -transfer -applications -lecture -chemical -organic -\n",
      "- Topic  4 : optical -optics -microscopy -imaging -epfl -electron -engineering -methods -laser -light -\n",
      "- Topic  5 : methods -materials -content -assessment -paper -teaching -work -activities -theory -outcomes -\n",
      "- Topic  6 : methods -cell -protein -molecular -flow -content -chemical -research -biology -structure -\n",
      "- Topic  7 : design -methods -data -analysis -structures -mechanics -materials -content -work -engineering -\n",
      "- Topic  8 : project -plan -skills -methods -data -programming -scientific -systems -content -software -\n",
      "- Topic  9 : systems -design -system -modeling -methods -circuits -processing -digital -techniques -concepts -\n",
      "- Topic  10 : models -linear -analysis -methods -control -data -algorithms -basic -statistical -statistics -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "displayTopicsWords([topic_indices], [\"default\"],[\"default\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha =  default & beta = default\n",
    "- financial risk : methods -models -model -stochastic -theory -time -introduction -exam -financial -risk -\n",
    "- energy management : methods -case -energy -management -content -work -assessment -outcomes -business -evaluate -\n",
    "- energy applications : methods -energy -equations -content -processes -transfer -applications -lecture -chemical -organic -\n",
    "- optical microscopy : optical -optics -microscopy -imaging -epfl -electron -engineering -methods -laser -light -\n",
    "- materials : methods -materials -content -assessment -paper -teaching -work -activities -theory -outcomes -\n",
    "- molecular research : methods -cell -protein -molecular -flow -content -chemical -research -biology -structure -\n",
    "- mechanics engineering : design -methods -data -analysis -structures -mechanics -materials -content -work -engineering -\n",
    "- software project : project -plan -skills -methods -data -programming -scientific -systems -content -software -\n",
    "- digital circuits modeling : systems -design -system -modeling -methods -circuits -processing -digital -techniques -concepts\n",
    "- linear algorithms : models -linear -analysis -methods -control -data -algorithms -basic -statistical -statistics -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it compare with LSI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.9: Dirichlet hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analyse the effects of α and β."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From : https://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda we have the following definitions:\n",
    "\n",
    "docConcentration (alpha): Dirichlet parameter for prior over documents’ distributions over topics. Larger values encourage smoother inferred distributions.\n",
    "\n",
    "topicConcentration (beta): Dirichlet parameter for prior over topics’ distributions over terms (words). Larger values encourage smoother inferred distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix k = 10 and β = 1.01, and vary α. How does it impact the topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When the alpha parameter is really high, the documents tend to be consisted of more topics (the probability the document is part of a topic tends to be equiprobable for all topics). We can see that when alpha = 20, the topics retrieved are pretty similar. These topics are retrieved instead of others because \"methods\" is a top word. On the other hand, when alpah = 1.01 is used, the top retrieved topics are the really specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createLdaTopics(alphaValues, betaValues):\n",
    "    topics_indices = []\n",
    "    for i in range(len(alphaValues)):\n",
    "        lda = LDA.train(documents, k = 10,  docConcentration = alphaValues[i], topicConcentration = betaValues[i])\n",
    "        topics_indices.append(lda.describeTopics(maxTermsPerTopic = 10))\n",
    "    return topics_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create lda model\n",
    "alphaValues1 = [20., 10., 6., 3., 1.01]\n",
    "betaValues1 = [1.01, 1.01, 1.01, 1.01, 1.01]\n",
    "lda_topics1 = createLdaTopics(alphaValues1, betaValues1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha =  20.0 & beta = 1.01\n",
      "Topic  1 : methods -content -management -project -work -solid -energy -analysis -assessment -teaching -\n",
      "Topic  2 : methods -design -content -data -project -analysis -systems -assessment -basic -outcomes -\n",
      "Topic  3 : methods -data -content -analysis -report -project -skills -outcomes -assessment -prerequisites -\n",
      "Topic  4 : methods -content -analysis -data -models -outcomes -control -assessment -basic -prerequisites -\n",
      "Topic  5 : design -methods -systems -analysis -content -concepts -week -system -assessment -teaching -\n",
      "Topic  6 : methods -risk -content -theory -basic -model -probability -introduction -assessment -concepts -\n",
      "Topic  7 : methods -content -outcomes -concepts -energy -basic -design -prerequisites -assessment -analysis -\n",
      "Topic  8 : methods -architecture -content -project -research -work -design -assessment -semester -teaching -\n",
      "Topic  9 : methods -materials -systems -content -organic -devices -exercises -basic -analysis -assessment -\n",
      "Topic  10 : methods -systems -engineering -design -analysis -content -teaching -system -assessment -data -\n",
      "alpha =  10.0 & beta = 1.01\n",
      "Topic  1 : methods -content -research -innovation -assessment -project -evaluate -activities -social -skills -\n",
      "Topic  2 : models -methods -theory -probability -stochastic -linear -exercises -analysis -content -exam -\n",
      "Topic  3 : systems -control -methods -system -data -power -processing -design -signal -electronics -\n",
      "Topic  4 : design -methods -image -cell -digital -structures -week -basic -structural -analysis -\n",
      "Topic  5 : methods -equations -flow -numerical -models -molecular -analysis -basic -problems -protein -\n",
      "Topic  6 : data -methods -materials -risk -analysis -techniques -concepts -content -microscopy -basic -\n",
      "Topic  7 : methods -chemistry -content -systems -organic -concepts -outcomes -prerequisites -energy -properties -\n",
      "Topic  8 : methods -content -project -quantum -laser -work -semester -report -time -optical -\n",
      "Topic  9 : energy -methods -optical -content -design -principles -optics -devices -assessment -systems -\n",
      "Topic  10 : engineering -methods -project -teaching -systems -analysis -chemical -water -system -semester -\n",
      "alpha =  6.0 & beta = 1.01\n",
      "Topic  1 : methods -data -content -exercises -assessment -microscopy -skills -design -oral -basic -\n",
      "Topic  2 : chemistry -methods -chemical -organic -processes -applications -content -materials -reactions -modeling -\n",
      "Topic  3 : project -skills -plan -report -research -work -methods -data -scientific -based -\n",
      "Topic  4 : models -biology -methods -model -protein -paper -cell -molecular -content -exam -\n",
      "Topic  5 : methods -linear -analysis -theory -algorithms -processing -models -data -basic -optimization -\n",
      "Topic  6 : energy -materials -properties -power -physics -heat -phase -magnetic -transfer -equations -\n",
      "Topic  7 : design -methods -system -control -innovation -content -teaching -business -assessment -research -\n",
      "Topic  8 : systems -engineering -design -methods -exercises -content -lectures -expected -system -class -\n",
      "Topic  9 : methods -optical -quantum -risk -theory -laser -basic -optics -content -introduction -\n",
      "Topic  10 : design -methods -content -stability -week -digital -assessment -final -note -activities -\n",
      "alpha =  3.0 & beta = 1.01\n",
      "Topic  1 : methods -exam -assessment -work -theory -models -class -model -content -teaching -\n",
      "Topic  2 : chemical -chemistry -methods -processes -materials -magnetic -water -reactions -basic -content -\n",
      "Topic  3 : models -methods -analysis -linear -numerical -basic -concepts -exercises -equations -model -\n",
      "Topic  4 : methods -data -systems -algorithms -optimization -control -problems -analysis -system -design -\n",
      "Topic  5 : quantum -optical -materials -applications -content -methods -devices -mechanical -laser -design -\n",
      "Topic  6 : design -methods -teaching -development -work -group -research -project -content -skills -\n",
      "Topic  7 : energy -systems -design -methods -materials -concepts -heat -content -conversion -techniques -\n",
      "Topic  8 : methods -microscopy -optics -electron -content -imaging -optical -principles -mass -field -\n",
      "Topic  9 : scientific -biology -skills -information -project -methods -report -content -analysis -assessment -\n",
      "Topic  10 : project -semester -data -management -risk -engineering -research -systems -design -plan -\n",
      "alpha =  1.01 & beta = 1.01\n",
      "Topic  1 : design -project -methods -report -work -engineering -skills -semester -plan -week -\n",
      "Topic  2 : cell -biology -methods -molecular -biological -content -note -protein -cells -chemical -\n",
      "Topic  3 : design -methods -data -project -research -content -paper -work -presentation -assessment -\n",
      "Topic  4 : design -systems -methods -system -circuits -control -programming -content -processing -modeling -\n",
      "Topic  5 : optics -content -physics -applications -methods -optical -materials -properties -imaging -skills -\n",
      "Topic  6 : methods -quantum -materials -content -introduction -theory -image -physics -properties -magnetic -\n",
      "Topic  7 : linear -methods -models -basic -theory -analysis -probability -statistical -data -algebra -\n",
      "Topic  8 : data -methods -analysis -information -systems -management -class -project -concepts -content -\n",
      "Topic  9 : methods -numerical -models -analysis -project -time -equations -content -problems -introduction -\n",
      "Topic  10 : energy -methods -chemical -chemistry -processes -organic -heat -concepts -water -systems -\n"
     ]
    }
   ],
   "source": [
    "displayTopicsWords(lda_topics1, alphaValues1, betaValues1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix k = 10 and α = 6, and vary β. How does it impact the topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger is the parameter beta, more homogeneous become the topics. It is clearly the case when beta is equal to 20, almost all topics contain the same words. It does not happen when beta is small, like 1.01, the topics are more specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create lda model\n",
    "alphaValues2 = [6.,6.,6.,6.,6.]\n",
    "betaValues2 = [20., 10., 6., 3., 1.01]\n",
    "lda_topics2 = createLdaTopics(alphaValues2, betaValues2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha =  6.0  & beta =  20.0 : \n",
      "Topic  1 : methods -content -design -systems -analysis -end -assessment -outcomes -project -concepts -\n",
      "Topic  2 : methods -content -design -systems -analysis -end -assessment -data -outcomes -teaching -\n",
      "Topic  3 : methods -content -design -systems -analysis -end -assessment -outcomes -basic -teaching -\n",
      "Topic  4 : methods -content -design -analysis -end -systems -assessment -outcomes -basic -prerequisites -\n",
      "Topic  5 : methods -content -design -analysis -end -systems -assessment -outcomes -prerequisites -keywords -\n",
      "Topic  6 : methods -content -design -analysis -systems -end -assessment -outcomes -basic -data -\n",
      "Topic  7 : methods -content -systems -design -analysis -end -assessment -outcomes -basic -teaching -\n",
      "Topic  8 : methods -content -design -analysis -systems -end -assessment -outcomes -keywords -prerequisites -\n",
      "Topic  9 : methods -content -design -systems -analysis -end -assessment -outcomes -keywords -prerequisites -\n",
      "Topic  10 : methods -content -design -systems -analysis -end -assessment -outcomes -concepts -basic -\n",
      "\n",
      "alpha =  6.0  & beta =  10.0 : \n",
      "Topic  1 : methods -content -design -analysis -systems -end -assessment -outcomes -project -teaching -\n",
      "Topic  2 : methods -content -systems -design -analysis -basic -end -assessment -prerequisites -outcomes -\n",
      "Topic  3 : methods -content -design -analysis -systems -end -assessment -outcomes -basic -concepts -\n",
      "Topic  4 : methods -content -systems -design -analysis -end -assessment -basic -outcomes -prerequisites -\n",
      "Topic  5 : methods -content -design -analysis -systems -data -end -assessment -outcomes -project -\n",
      "Topic  6 : methods -content -systems -analysis -design -end -assessment -concepts -basic -teaching -\n",
      "Topic  7 : methods -content -design -systems -analysis -end -assessment -outcomes -teaching -project -\n",
      "Topic  8 : methods -design -content -systems -analysis -end -project -assessment -data -outcomes -\n",
      "Topic  9 : methods -content -design -end -systems -analysis -assessment -outcomes -basic -prerequisites -\n",
      "Topic  10 : methods -content -design -systems -analysis -end -assessment -outcomes -teaching -prerequisites -\n",
      "\n",
      "alpha =  6.0  & beta =  6.0 : \n",
      "Topic  1 : methods -project -content -design -analysis -data -end -assessment -work -skills -\n",
      "Topic  2 : methods -content -analysis -systems -design -end -basic -outcomes -prerequisites -assessment -\n",
      "Topic  3 : methods -content -energy -analysis -systems -end -assessment -outcomes -basic -prerequisites -\n",
      "Topic  4 : methods -content -systems -analysis -design -end -outcomes -assessment -concepts -basic -\n",
      "Topic  5 : methods -systems -design -content -analysis -basic -end -concepts -outcomes -assessment -\n",
      "Topic  6 : methods -content -design -project -end -assessment -work -analysis -data -teaching -\n",
      "Topic  7 : methods -content -systems -assessment -energy -end -basic -keywords -analysis -models -\n",
      "Topic  8 : methods -design -data -content -project -systems -analysis -end -assessment -outcomes -\n",
      "Topic  9 : methods -content -materials -design -assessment -basic -end -keywords -prerequisites -outcomes -\n",
      "Topic  10 : methods -systems -content -design -analysis -end -assessment -basic -teaching -keywords -\n",
      "\n",
      "alpha =  6.0  & beta =  3.0 : \n",
      "Topic  1 : methods -materials -content -design -end -project -analysis -assessment -work -outcomes -\n",
      "Topic  2 : methods -content -systems -design -end -assessment -keywords -basic -prerequisites -teaching -\n",
      "Topic  3 : methods -content -systems -end -assessment -outcomes -introduction -concepts -basic -teaching -\n",
      "Topic  4 : methods -content -design -analysis -end -outcomes -concepts -basic -prerequisites -teaching -\n",
      "Topic  5 : methods -theory -basic -content -analysis -design -assessment -ii -end -models -\n",
      "Topic  6 : methods -content -overview -assessment -design -keywords -end -teaching -research -analysis -\n",
      "Topic  7 : methods -design -models -systems -analysis -modeling -content -data -processing -end -\n",
      "Topic  8 : methods -chemistry -materials -content -chemical -energy -properties -basic -end -assessment -\n",
      "Topic  9 : systems -design -methods -system -data -project -energy -control -analysis -end -\n",
      "Topic  10 : methods -models -analysis -data -time -content -aspects -project -end -plan -\n",
      "\n",
      "alpha =  6.0  & beta =  1.01 : \n",
      "Topic  1 : energy -design -systems -methodology -methods -devices -circuits -control -term -end -\n",
      "Topic  2 : methods -data -basic -programming -content -concepts -information -theory -probability -management -\n",
      "Topic  3 : methods -process -content -teaching -research -presentation -overview -epfl -assessment -analysis -\n",
      "Topic  4 : methods -report -analysis -aspects -project -skills -plan -based -models -model -\n",
      "Topic  5 : methods -content -processes -models -analysis -basic -linear -exercises -lecture -prerequisites -\n",
      "Topic  6 : methods -modeling -magnetic -molecular -exercises -properties -content -physics -project -applications -\n",
      "Topic  7 : chemical -organic -methods -content -context -chemistry -state -assessment -properties -reactions -\n",
      "Topic  8 : methods -processing -linear -phase -analysis -basic -models -content -work -exercises -\n",
      "Topic  9 : methods -electron -content -ii -materials -keywords -energy -systems -introduction -prerequisites -\n",
      "Topic  10 : design -systems -optical -methods -data -project -optics -engineering -innovation -work -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "displayTopicsWords(lda_topics2, alphaValues2, betaValues2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.10: EPFL's taught subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### List the subjects of EPFL’s classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the combination of k, α and β that gives most interpretable topics. Explain why you chose these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose k = 30, alpha = 1.01 and beta = 1.01 because :\n",
    "- EPFL has documents about lots of topics, so k needs to be high enough to create sufficiently enough topics\n",
    "- alpha should be small enough, since subject of EPFL's classes are usually containing only one topic. \n",
    "- beta should be small, so that topics are not too similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha =  1.01 & beta = 1.01\n",
      "Topic  1 : design -data -systems -system -methods -programming -modeling -digital -tools -teaching -\n",
      "Topic  2 : magnetic -methods -cell -materials -content -drug -cells -note -molecular -introduction -\n",
      "Topic  3 : materials -chemical -chemistry -methods -properties -protein -structure -molecular -reaction -content -\n",
      "Topic  4 : methods -microscopy -electron -design -content -business -analysis -keywords -assessment -class -\n",
      "Topic  5 : methods -skills -transversal -work -content -concepts -outcomes -assessment -presentation -physical -\n",
      "Topic  6 : models -methods -theory -model -time -stochastic -financial -risk -finance -heat -\n",
      "Topic  7 : quantum -methods -content -theory -properties -basic -outcomes -systems -prerequisites -snow -\n",
      "Topic  8 : energy -project -methods -plan -skills -process -design -systems -conversion -outcomes -\n",
      "Topic  9 : methods -circuits -content -design -systems -noise -basic -devices -exercises -organic -\n",
      "Topic  10 : data -project -methods -paper -assessment -work -group -content -research -activities -\n"
     ]
    }
   ],
   "source": [
    "# create lda model\n",
    "lda3 = LDA.train(documents, k = 15, docConcentration = 1.01, topicConcentration = 1.01)\n",
    "\n",
    "# get topics\n",
    "lda_topics3 = lda3.describeTopics(maxTermsPerTopic = 10) # 10 words to display\n",
    "displayTopicsWords([lda_topics3], [1.01],[1.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the values of these hyperparameters that you used and your labels for the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "k = 15 & alpha =  1.01  & beta =  1.01 : \n",
    "- digital tools : \n",
    "design -data -systems -system -methods -programming -modeling -digital -tools -teaching -\n",
    "- magnetic cells : \n",
    "magnetic -methods -cell -materials -content -drug -cells -note -molecular -introduction -\n",
    "- chemical properties of protein : \n",
    "materials -chemical -chemistry -methods -properties -protein -structure -molecular -reaction -content -\n",
    "- electron analysis : \n",
    " methods -microscopy -electron -design -content -business -analysis -keywords -assessment -class -\n",
    "- transversal skills : \n",
    " methods -skills -transversal -work -content -concepts -outcomes -assessment -presentation -physical -\n",
    "- stochastic models in finance : \n",
    "models -methods -theory -model -time -stochastic -financial -risk -finance -heat -\n",
    "- quantum methods : \n",
    "quantum -methods -content -theory -properties -basic -outcomes -systems -prerequisites -snow -\n",
    "- energy systems : \n",
    " energy -project -methods -plan -skills -process -design -systems -conversion -outcomes -\n",
    "- circuits and systems noise : \n",
    "methods -circuits -content -design -systems -noise -basic -devices -exercises -organic -\n",
    "- project group : \n",
    "data -project -methods -paper -assessment -work -group -content -research -activities -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.11: Wikipedia structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Extract the structure in terms of topics from the wikipedia-for-school dataset. Use your intuition about how many topics might be covered by the articles and how they are distributed. Report the values for k, α and β that you chose a priori and why you picked them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at all the different words, we can see that there exists words in different languages. Thus the topics which should be retrieved should take into accounts the languages, leading to a less strict beta variable. Alpha variable should also be a bit increased since wikipedia pages may contain several topics (generally a wikipedia page can collect various topics). The variable k is also increased since wikipedia can contain a very large set of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "wikipedia = sc.textFile(\"/ix/wikipedia-for-schools.txt\").map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Group words in order to obtain a word list\n",
    "    #add a counter\n",
    "    #reducebykey to group words\n",
    "    #descending sort on number of recurrence\n",
    "mostReccurentWords1 = wikipedia.map(lambda x: x[\"tokens\"])\\\n",
    ".map(lambda w : [x for x in w if x.isalpha()])\\\n",
    ".map(lambda w : [x for x in w if len(x)>3])\\\n",
    ".flatMap(lambda x: x)\\\n",
    ".map(lambda word: (word, 1))\\\n",
    ".reduceByKey(lambda x,y: x + y)\\\n",
    ".map(lambda x: (x[1], x[0]))\\\n",
    ".sortByKey(False)\n",
    "\n",
    "    \n",
    "# get all possible words and map them to an id\n",
    "wordsList1 = mostReccurentWords1.map(lambda x: x[1]).zipWithIndex().collectAsMap()\n",
    "\n",
    "distinctNbWords1 = len(wordsList1)\n",
    "\n",
    "    \n",
    "# get a document and return a tuple (id , vector of word occurrencies )\n",
    "def documentToVector1(d):\n",
    "    \n",
    "    wordOccurrencies = defaultdict(int) #initialize dict\n",
    "    for w in d[0]: \n",
    "        wordOccurrencies[wordsList1[w]] += 1 # add one at word_id position\n",
    "            \n",
    "    wordOccurrencies = sorted(wordOccurrencies.items()) # in order to obtain a list for each element\n",
    "    wordOcc_0 = [x[0] for x in wordOccurrencies]\n",
    "    wordOcc_1 = [x[1] for x in wordOccurrencies]\n",
    "    \n",
    "    return (d[1], Vectors.sparse(distinctNbWords1, wordOcc_0, wordOcc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform indices into corresponding words\n",
    "wordsList_helper1 = mostReccurentWords1.map(lambda x: x[1]).collect()\n",
    "\n",
    "# get right format to submit to lda\n",
    "    # zipWithIndex to obtain an id for the document\n",
    "    # get a vector of used words\n",
    "documents1 = wikipedia.map(lambda x: x[\"tokens\"]).zipWithIndex().map(documentToVector1).map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o741.trainLDAModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8126.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8126.0 (TID 3633, iccluster122.iccluster.epfl.ch): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/2.5.3.0-37/spark/python/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/hdp/2.5.3.0-37/spark/python/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/2.5.3.0-37/spark/python/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-48-d0cd5f05d4a6>\", line 19, in documentToVector1\nKeyError: 'mac'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1433)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1420)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1420)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:801)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:801)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:801)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1590)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:622)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1856)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1869)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1882)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1335)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1309)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.initialize(LDAOptimizer.scala:141)\n\tat org.apache.spark.mllib.clustering.LDA.run(LDA.scala:324)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLDAModel(PythonMLLibAPI.scala:538)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/2.5.3.0-37/spark/python/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/hdp/2.5.3.0-37/spark/python/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/2.5.3.0-37/spark/python/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-48-d0cd5f05d4a6>\", line 19, in documentToVector1\nKeyError: 'mac'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-57d71fb97608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mk_wiki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlda_wiki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocConcentration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphaValue_wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopicConcentration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbetaValue_wiki\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# get topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/2.5.3.0-37/spark/python/pyspark/mllib/clustering.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, rdd, k, maxIterations, docConcentration, topicConcentration, seed, checkpointInterval, optimizer)\u001b[0m\n\u001b[1;32m    802\u001b[0m         model = callMLlibFunc(\"trainLDAModel\", rdd, k, maxIterations,\n\u001b[1;32m    803\u001b[0m                               \u001b[0mdocConcentration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopicConcentration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m                               checkpointInterval, optimizer)\n\u001b[0m\u001b[1;32m    805\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLDAModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/2.5.3.0-37/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/2.5.3.0-37/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/2.5.3.0-37/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o741.trainLDAModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8126.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8126.0 (TID 3633, iccluster122.iccluster.epfl.ch): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/2.5.3.0-37/spark/python/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/hdp/2.5.3.0-37/spark/python/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/2.5.3.0-37/spark/python/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-48-d0cd5f05d4a6>\", line 19, in documentToVector1\nKeyError: 'mac'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1433)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1420)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1420)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:801)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:801)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:801)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1590)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:622)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1856)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1869)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1882)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1335)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1309)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.initialize(LDAOptimizer.scala:141)\n\tat org.apache.spark.mllib.clustering.LDA.run(LDA.scala:324)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLDAModel(PythonMLLibAPI.scala:538)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/2.5.3.0-37/spark/python/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/hdp/2.5.3.0-37/spark/python/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/2.5.3.0-37/spark/python/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-48-d0cd5f05d4a6>\", line 19, in documentToVector1\nKeyError: 'mac'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:277)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# create lda model\n",
    "alphaValue_wiki = 3.\n",
    "betaValue_wiki = 2.\n",
    "k_wiki = 50\n",
    "\n",
    "lda_wiki = LDA.train(documents1, k = k_wiki, docConcentration = alphaValue_wiki, topicConcentration = betaValue_wiki)\n",
    "\n",
    "# get topics\n",
    "topic_indices_wiki = lda_wiki.describeTopics(maxTermsPerTopic = 10) # 10 words to display\n",
    "displayTopicsWords(topic_indices_wiki, [alphaValue_wiki], [betaValue_wiki])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are you convinced by the results? Give labels to the topics if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
