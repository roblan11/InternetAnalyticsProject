{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** K\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* Kim Lan Phan Hoang\n",
    "* Robin Lang\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')\n",
    "punctuation = '.?!,;:-–()[]{}\"/\\'0123456789%*+='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pre-process the corpus to create bag-of-words representations of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN = 10\n",
    "MAX = 2000\n",
    "\n",
    "# note: downloading wordnet is required\n",
    "# command: python -m nltk.downloader\n",
    "#   in the console\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "def find_ngrams(input_list, n):\n",
    "    return [str(x[0] + \" \" + x[1]) for x in list(zip(*[input_list[i:] for i in range(n)])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "courses_proc = {}\n",
    "words = {}\n",
    "cids = []\n",
    "\n",
    "for c in courses:\n",
    "    cid = c['courseId']\n",
    "    cids.append(cid)\n",
    "    \n",
    "    # transfer all to lowercase, remove punctuation\n",
    "    desc = c['description'].lower().translate(str.maketrans('', '', punctuation))\n",
    "    # remove stopwords\n",
    "    desc_proc = [lemmatizer.lemmatize(word, pos='v') for word in desc.split() if word not in stopwords]\n",
    "    \n",
    "    desc_2grams = find_ngrams(desc_proc, 2)\n",
    "    desc_proc.extend(desc_2grams)\n",
    "    \n",
    "    # create a dict of all words\n",
    "    for w in desc_proc:\n",
    "        if w in words:\n",
    "            words[w] += 1\n",
    "        else:\n",
    "            words[w] = 1\n",
    "    \n",
    "    if cid in courses_proc:\n",
    "        # some courseIds appear multiple times\n",
    "        # for those, we decided to append the desctiptions to each other\n",
    "        courses_proc[cid].extend(desc_proc)\n",
    "    else:\n",
    "        courses_proc[cid] = desc_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_filtered = []\n",
    "\n",
    "# most and least frequent words to filter\n",
    "for w in words:\n",
    "    if words[w] > MIN and words[w] < MAX:\n",
    "        words_filtered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "courses_proc2 = {}\n",
    "\n",
    "for c in courses_proc:\n",
    "    courses_proc2[c] = [word for word in courses_proc[c] if word in words_filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain which kinds of cleaning you implemented and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Make sure every courseId only appears once. If it appears more than once, append the two descriptions\n",
    "* Change all cheacters to lowercase, to avoid the same word being considered different. Additionally, all words in stopwords.pkl are in lowercase, so this ensures all words are removed correctly\n",
    "* Remove punctuation, numbers and other symbols, for the same reason as above. Only the words are important, not which one is before a comma.\n",
    "* Remove stopwords, as these don't carry any information about the content of the document\n",
    "* filter most and least frequent words, as they only mess up the results when trying to find similarities between documents. commomn words will be in almost every document, rare ones in almost none.\n",
    "* Lemmatization, to detect only words with different meaning. Identical words, such as \"is\" and \"are\" will automatically be transformed to \"be\", the same is true with multiples (\"documents\" -> \"document\", \"are\" -> \"be\", ...).\n",
    "* n-grams: 2-shingles, to detect frequent combinations of words. Infrequent shingles are filtered just like infrequent words. No higher degrees for the sake of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the terms in the pre-processed description of the $9^{th}$ class in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract', 'abstract', 'abstract', 'activities', 'activities', 'activities', 'activities attend', 'activities attend', 'activities attend', 'al', 'al', 'al', 'algorithmic', 'algorithmic', 'algorithmic', 'architecture', 'architecture', 'architecture', 'assessment', 'assessment', 'assessment', 'assessment methods', 'assessment methods', 'assessment methods', 'assistants', 'assistants', 'assistants', 'assistants forum', 'assistants forum', 'assistants forum', 'attend', 'attend', 'attend', 'attend lecture', 'attend lecture', 'attend lecture', 'automation', 'automation', 'automation', 'basic', 'basic', 'basic', 'bibliography', 'bibliography', 'bibliography', 'bibliothèque', 'bibliothèque', 'bibliothèque', 'build', 'build', 'build', 'challenge', 'challenge', 'challenge', 'cod', 'cod', 'cod', 'combinational', 'combinational', 'combinational', 'comment', 'comment', 'comment', 'complete', 'complete', 'complete', 'complete exercise', 'complete exercise', 'complete exercise', 'complex', 'complex', 'complex', 'components', 'components', 'components', 'computation', 'computation', 'computation', 'concepts', 'concepts', 'concepts', 'concepts', 'concepts', 'concepts', 'concepts start', 'concepts start', 'concepts start', 'content introduction', 'content introduction', 'content introduction', 'control', 'control', 'control', 'creation', 'creation', 'creation', 'cs', 'cs', 'cs', 'de', 'de', 'de', 'definition', 'definition', 'definition', 'deliver', 'deliver', 'deliver', 'describe', 'describe', 'describe', 'description', 'description', 'description', 'description', 'description', 'description', 'design automation', 'design automation', 'design automation', 'design control', 'design control', 'design control', 'design methodologies', 'design methodologies', 'design methodologies', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'develop', 'develop', 'develop', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital hardware', 'digital hardware', 'digital hardware', 'digital hardware', 'digital hardware', 'digital hardware', 'digital hardware', 'digital hardware', 'digital hardware', 'digital systems', 'digital systems', 'digital systems', 'digital systems', 'digital systems', 'digital systems', 'ed', 'ed', 'ed', 'eda', 'eda', 'eda', 'eda', 'eda', 'eda', 'eda tool', 'eda tool', 'eda tool', 'eda tool', 'eda tool', 'eda tool', 'ee', 'ee', 'ee', 'ee', 'ee', 'ee', 'efficiency', 'efficiency', 'efficiency', 'efficient', 'efficient', 'efficient', 'electronic', 'electronic', 'electronic', 'electronic', 'electronic', 'electronic', 'elements', 'elements', 'elements', 'elements', 'elements', 'elements', 'embed', 'embed', 'embed', 'embed', 'embed', 'embed', 'embed systems', 'embed systems', 'embed systems', 'embed systems', 'embed systems', 'embed systems', 'en', 'en', 'en', 'en bibliothèque', 'en bibliothèque', 'en bibliothèque', 'end student', 'end student', 'end student', 'environment', 'environment', 'environment', 'environment', 'environment', 'environment', 'environment', 'environment', 'environment', 'essential', 'essential', 'essential', 'examination', 'examination', 'examination', 'examination', 'examination', 'examination', 'examination final', 'examination final', 'examination final', 'exercise', 'exercise', 'exercise', 'exercise', 'exercise', 'exercise', 'exercise', 'exercise', 'exercise', 'exercise expect', 'exercise expect', 'exercise expect', 'expect', 'expect', 'expect', 'expect student', 'expect student', 'expect student', 'feature', 'feature', 'feature', 'feedback', 'feedback', 'feedback', 'final', 'final', 'final', 'final examination', 'final examination', 'final examination', 'formalisms', 'formalisms', 'formalisms', 'forum', 'forum', 'forum', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'fundamental', 'fundamental', 'fundamental', 'guide', 'guide', 'guide', 'guide', 'guide', 'guide', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware description', 'hardware description', 'hardware description', 'hardware description', 'hardware description', 'hardware description', 'hardware model', 'hardware model', 'hardware model', 'homework', 'homework', 'homework', 'homework exercise', 'homework exercise', 'homework exercise', 'hours', 'hours', 'hours', 'hours assistants', 'hours assistants', 'hours assistants', 'httpmoodleepflchcourseviewphpid', 'httpmoodleepflchcourseviewphpid', 'httpmoodleepflchcourseviewphpid', 'important', 'important', 'important', 'important concepts', 'important concepts', 'important concepts', 'include', 'include', 'include', 'individual', 'individual', 'individual', 'integrate', 'integrate', 'integrate', 'integrate exercise', 'integrate exercise', 'integrate exercise', 'introduce', 'introduce', 'introduce', 'introduction', 'introduction', 'introduction', 'issue', 'issue', 'issue', 'keywords', 'keywords', 'keywords', 'lab', 'lab', 'lab', 'language', 'language', 'language', 'language', 'language', 'language', 'language', 'language', 'language', 'languages', 'languages', 'languages', 'languages', 'languages', 'languages', 'layer', 'layer', 'layer', 'learn outcomes', 'learn outcomes', 'learn outcomes', 'learn prerequisites', 'learn prerequisites', 'learn prerequisites', 'lecture complete', 'lecture complete', 'lecture complete', 'lecture integrate', 'lecture integrate', 'lecture integrate', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'link', 'link', 'link', 'link httpmoodleepflchcourseviewphpid', 'link httpmoodleepflchcourseviewphpid', 'link httpmoodleepflchcourseviewphpid', 'logic', 'logic', 'logic', 'logic systems', 'logic systems', 'logic systems', 'methodologies', 'methodologies', 'methodologies', 'methodologies', 'methodologies', 'methodologies', 'methodology', 'methodology', 'methodology', 'methods homework', 'methods homework', 'methods homework', 'methods lecture', 'methods lecture', 'methods lecture', 'midterm', 'midterm', 'midterm', 'midterm examination', 'midterm examination', 'midterm examination', 'model concepts', 'model concepts', 'model concepts', 'model digital', 'model digital', 'model digital', 'model formalisms', 'model formalisms', 'model formalisms', 'model level', 'model level', 'model level', 'model model', 'model model', 'model model', 'modern', 'modern', 'modern', 'moodle', 'moodle', 'moodle', 'moodle', 'moodle', 'moodle', 'moodle link', 'moodle link', 'moodle link', 'moodle page', 'moodle page', 'moodle page', 'note', 'note', 'note', 'noteshandbook', 'noteshandbook', 'noteshandbook', 'notion', 'notion', 'notion', 'office', 'office', 'office', 'office hours', 'office hours', 'office hours', 'open', 'open', 'open', 'outcomes', 'outcomes', 'outcomes', 'outcomes end', 'outcomes end', 'outcomes end', 'page', 'page', 'page', 'plan', 'plan', 'plan', 'prerequisites', 'prerequisites', 'prerequisites', 'prerequisites require', 'prerequisites require', 'prerequisites require', 'principles', 'principles', 'principles', 'problems', 'problems', 'problems', 'proper', 'proper', 'proper', 'proper', 'proper', 'proper', 'quiz', 'quiz', 'quiz', 'recommend', 'recommend', 'recommend', 'recommend course', 'recommend course', 'recommend course', 'require', 'require', 'require', 'require', 'require', 'require', 'require course', 'require course', 'require course', 'resources', 'resources', 'resources', 'resources bibliography', 'resources bibliography', 'resources bibliography', 'ressources', 'ressources', 'ressources', 'ressources en', 'ressources en', 'ressources en', 'reusable', 'reusable', 'reusable', 'review', 'review', 'review', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'sequential', 'sequential', 'sequential', 'soc', 'soc', 'soc', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'source', 'source', 'source', 'springer', 'springer', 'springer', 'springer', 'springer', 'springer', 'start', 'start', 'start', 'stateoftheart', 'stateoftheart', 'stateoftheart', 'student activities', 'student activities', 'student activities', 'student describe', 'student describe', 'student describe', 'supervision', 'supervision', 'supervision', 'supervision office', 'supervision office', 'supervision office', 'synthesis', 'synthesis', 'synthesis', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system design', 'system design', 'system design', 'system level', 'system level', 'system level', 'system level', 'system level', 'system level', 'system level', 'system level', 'system level', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systems cs', 'systems cs', 'systems cs', 'systems design', 'systems design', 'systems design', 'systems design', 'systems design', 'systems design', 'systemsonchip', 'systemsonchip', 'systemsonchip', 'systemsonchip', 'systemsonchip', 'systemsonchip', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'task', 'task', 'task', 'teach', 'teach', 'teach', 'teach methods', 'teach methods', 'teach methods', 'techniquesdevelop', 'techniquesdevelop', 'techniquesdevelop', 'tool', 'tool', 'tool', 'tool', 'tool', 'tool', 'users', 'users', 'users', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'websites', 'websites', 'websites', 'work', 'work', 'work']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(courses_proc2[courses[9]['courseId']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Construct an M×N term-document matrix X, where M is the number of terms and N is the number of documents. The matrix X should be sparse. You are not allowed to use libraries for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mapping from courseId to its index\n",
    "words_index = {}\n",
    "i = 0\n",
    "words_list = []\n",
    "\n",
    "for w in words_filtered:\n",
    "    words_index[w] = i\n",
    "    i += 1\n",
    "    words_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapping from courseId to its index\n",
    "courses_index = {}\n",
    "i = 0\n",
    "courses_list = []\n",
    "\n",
    "for w in courses_proc2:\n",
    "    courses_index[w] = i\n",
    "    i += 1\n",
    "    courses_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = len(words_index)\n",
    "N = len(courses_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.ndarray((M, N))\n",
    "\n",
    "for c in courses_proc2:\n",
    "    for t in courses_proc2[c]:\n",
    "        X[words_index[t]][courses_index[c]] += 1\n",
    "        if courses_index[c]==0:\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the 15 terms in the description of the $9^{th}$ class with the highest TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TF = np.array([x / x.max() for x in X.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IDF = np.ndarray(M)\n",
    "for i in range(len(X)):\n",
    "    count = 0\n",
    "    for j in X[i]:\n",
    "        if j > 0:\n",
    "            count += 1\n",
    "    IDF[i] = np.log2(N / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TFIDF = np.array([x * IDF for x in TF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IFIDF_9 = TFIDF[courses_index[courses[9]['courseId']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TFIDF_9_top15 = np.argsort(-IFIDF_9)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 15 TERMS FROM TFIDF\n",
      "  6.03765254148 : verification\n",
      "  3.89523690385 : systemverilog\n",
      "  3.89523690385 : systemc\n",
      "  3.89523690385 : functional verification\n",
      "  3.81500253808 : vhdl\n",
      "  3.26125190356 : rtl\n",
      "  2.59682460257 : systemverilog systemc\n",
      "  2.49469492386 : hardware\n",
      "  2.3301579359 : socs\n",
      "  2.3301579359 : design vhdl\n",
      "  1.87141350903 : functional\n",
      "  1.74761845192 : digital hardware\n",
      "  1.56208089818 : digital\n",
      "  1.54761845192 : system level\n",
      "  1.08708396785 : systemsonchip\n"
     ]
    }
   ],
   "source": [
    "print(\"TOP 15 TERMS FROM TFIDF\")\n",
    "for i in TFIDF_9_top15:\n",
    "    print(\" \", IFIDF_9[i], \":\", words_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain where the difference between the large scores and the small ones comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TF gives the importance if a word in a document, between 0 and 1. 1 is the most frequent word, 0 means they never appear.\n",
    "\n",
    "IDF gives the frequency of a word in the corpus, bigger than 0. 0 means the word is in every description, infinity means it's in none of them.\n",
    "\n",
    "TFIDF then is simply the mutliplication of the two, where a high TFIDF means that word is frequent within the desctiption, but rare within the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Search for \"markov chains\" and \"facebook\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the word \"chains\" has been replaced by \"chain\" by lemmatization\n",
    "markov_chain_index = words_index['markov chain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'facebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-aea2ef02685d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'facebook'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'facebook'"
     ]
    }
   ],
   "source": [
    "words_index['facebook']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the term \"facebook\" only appeeared 9 times within only one description, and was therefore filtered eaclier. thus, that term has no score and no similarity can be computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the top five courses together with their similarity score for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim(i, j):\n",
    "    di = TFIDF[i]\n",
    "    dj = TFIDF[j]\n",
    "    return np.dot(di, dj) / (np.linalg.norm(di) * np.linalg.norm(dj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "markov_chain_score = []\n",
    "\n",
    "for i in TFIDF:\n",
    "    markov_chain_score.append(i[markov_chain_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 15 TERMS FROM TFIDF\n",
      "  5.94063200363 : MATH-332\n",
      "  4.62049155838 : COM-516\n",
      "  3.46536866878 : MGT-484\n",
      "  1.26013406138 : FIN-408\n",
      "  1.15512288959 : MATH-600\n"
     ]
    }
   ],
   "source": [
    "TFIDF_markov_top5 = np.argsort(-np.array(markov_chain_score))[:5]\n",
    "top_markov_names = []\n",
    "\n",
    "print(\"TOP 15 TERMS FROM TFIDF\")\n",
    "for i in TFIDF_markov_top5:\n",
    "    top_markov_names.append([i, courses_list[i]])\n",
    "    print(\" \", markov_chain_score[i], \":\", courses_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITIES BETWEEN TOP 5 MARKOV CHAINS SCORES\n",
      "          MATH-332 COM-516 MGT-484 FIN-408 MATH-600\n",
      "MATH-332    1.00    0.34    0.33    0.18    0.14\n",
      " COM-516    0.34    1.00    0.32    0.21    0.19\n",
      " MGT-484    0.33    0.32    1.00    0.18    0.10\n",
      " FIN-408    0.18    0.21    0.18    1.00    0.04\n",
      "MATH-600    0.14    0.19    0.10    0.04    1.00\n"
     ]
    }
   ],
   "source": [
    "print(\"SIMILARITIES BETWEEN TOP 5 MARKOV CHAINS SCORES\")\n",
    "print(\"         \", top_markov_names[0][1], top_markov_names[1][1], top_markov_names[2][1],\n",
    "      top_markov_names[3][1], top_markov_names[4][1])\n",
    "for i in top_markov_names:\n",
    "    line = \"\" + i[1].rjust(8)\n",
    "    for j in top_markov_names:\n",
    "        line += \"    \" + str(\"%.2f\" % round(sim(i[0], j[0]), 2))\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you think of the results? Give your intuition on what is happening.\n",
    "\n",
    "Since all we looked for were the descriptions where \"markov chain(s)\" appears the most, we expected to have relatively low similartiy scores. It is likely that courses that deal with markov chains have somewhat similar topics, as it is a mathematical concept at the base, but that is not at all reqired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"data/TFIDF.npy\", TFIDF)\n",
    "np.save(\"data/words.npy\", words_list)\n",
    "np.save(\"data/words_dict.npy\", words_index)\n",
    "np.save(\"data/courses.npy\", courses_list)\n",
    "np.save(\"data/courses_dict.npy\", courses_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
