{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** K\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* Kim Lan Phan Hoang\n",
    "* Robin Lang\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')\n",
    "punctuation = '.?!,;:-–()[]{}\"/\\''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pre-process the corpus to create bag-of-words representations of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN = 10\n",
    "MAX = 2000\n",
    "\n",
    "# note: downloading wordnet is required\n",
    "# command: python -m nltk.downloader\n",
    "#   in the console\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "courses_proc = {}\n",
    "words = {}\n",
    "cids = []\n",
    "\n",
    "for c in courses:\n",
    "    cid = c['courseId']\n",
    "    cids.append(cid)\n",
    "    \n",
    "    # transfer all to lowercase, remove punctuation\n",
    "    desc = c['description'].lower().translate(str.maketrans('', '', punctuation))\n",
    "    # remove stopwords\n",
    "    desc_proc = [lemmatizer.lemmatize(word, pos='v') for word in desc.split() if word not in stopwords]\n",
    "    desc_2grams = find_ngrams(desc_proc, 2)\n",
    "    \n",
    "    desc_proc.extend(desc_2grams)\n",
    "    \n",
    "    # create a dict of all words\n",
    "    for w in desc_proc:\n",
    "        if w in words:\n",
    "            words[w] += 1\n",
    "        else:\n",
    "            words[w] = 1\n",
    "    \n",
    "    # not needed ?\n",
    "    #name = c['name']\n",
    "    #courses_proc[cid] = desc_proc\n",
    "    \n",
    "    if cid in courses_proc:\n",
    "        # some courseIds appear multiple times\n",
    "        # for those, we decided to append the desctiptions to each other\n",
    "        courses_proc[cid].extend(desc_proc)\n",
    "    else:\n",
    "        courses_proc[cid] = desc_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_filtered = []\n",
    "\n",
    "# most and least frequent words to filter\n",
    "for w in words:\n",
    "    if words[w] > MIN or words[w] < MAX:\n",
    "        words_filtered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "courses_proc2 = {}\n",
    "\n",
    "for c in courses_proc:\n",
    "    courses_proc2[c] = [word for word in courses_proc[c] if word in words_filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain which kinds of cleaning you implemented and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* make sure every courseId only appears once. If it appears more than once, append the two descriptions\n",
    "* Put all cheacters to lowercase, to avoid the same word being considered different. Additionally, all words in stopwords.pkl are in lowercase, so this ensures all words are removed correctly\n",
    "* Remove punctuation, for the same reason as above. Only the words are important, not which one is before a comma.\n",
    "* Remove stopwords, as these don't carry any information about the content of the document\n",
    "* filter most and least frequent words, as they only mess up the results when trying to find similarities between documents. commomn words will be in almost every document, rare ones in almost none.\n",
    "* Lemmatization, to detect only words with different meaning. Identical words, such as \"is\" and \"are\" will automatically be transformed to \"be\", the same is true with multiples (\"documents\" -> \"document\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the terms in the pre-processed description of the $9^{th}$ class in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sorted(courses_proc2[courses[9]['courseId']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Construct an M×N term-document matrix X, where M is the number of terms and N is the number of documents. The matrix X should be sparse. You are not allowed to use libraries for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mapping from word to its index\n",
    "words_index = {}\n",
    "i = 0\n",
    "\n",
    "for w in words_filtered:\n",
    "    words_index[w] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapping from courseId to its index\n",
    "courses_index = {}\n",
    "i = 0\n",
    "\n",
    "for w in courses_proc2:\n",
    "    courses_index[w] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = len(words_filtered)\n",
    "N = len(courses_proc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.ndarray((M, N))\n",
    "\n",
    "for c in courses_proc2:\n",
    "    for t in courses_proc2[c]:\n",
    "        X[words_index[t]][courses_index[c]] += 1\n",
    "        if courses_index[c]==0:\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the 15 terms in the description of the $9^{th}$ class with the highest TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TF = np.array([x / x.max() for x in X.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IDF = np.ndarray(M)\n",
    "for i in range(len(X)):\n",
    "    count = 0\n",
    "    for j in X[i]:\n",
    "        if j > 0:\n",
    "            count += 1\n",
    "    IDF[i] = np.log2(N / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TFIDF = np.array([x * IDF for x in TF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IFIDF_9 = TFIDF[courses_index[courses[9]['courseId']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TFIDF_9_top15 = np.argsort(-IFIDF_9)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"TOP 15 TERMS FROM TFIDF\")\n",
    "for i in TFIDF_9_top15:\n",
    "    for k, v in words_index.items():\n",
    "        if v == i:\n",
    "            print(\" \", IFIDF_9[i], \":\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain where the difference between the large scores and the small ones comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TF gives the importance if a word in a document, between 0 and 1. 1 is the most frequent word, 0 means they never appear.\n",
    "\n",
    "IDF gives the frequency of a word in the corpus, bigger than 0. 0 means the word is in every description, infinity means it's in none of them.\n",
    "\n",
    "TFIDF then is simply the mutliplication of the two, where a high TFIDF means that word is frequent within the desctiption, but rare within the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Search for \"markov chains\" and \"facebook\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markov_index = words_index['markov']\n",
    "chains_index = words_index['chains']\n",
    "facebook_index = words_index['facebook']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the top five courses together with their similarity score for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you think of the results? Give your intuition on what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
