{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** K\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* Kim Lan Phan Hoang\n",
    "* Robin Lang\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# load the files and initialize the punctiuation to remove\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')\n",
    "punctuation = '.?!,;:-–()[]{}\"/\\'0123456789%*+='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pre-process the corpus to create bag-of-words representations of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the minimum and maximum times any term may appear\n",
    "# terms that occur less than MIN\n",
    "# or more than MAX times get filtered\n",
    "MIN = 10\n",
    "MAX = 2000\n",
    "\n",
    "# note: downloading wordnet is required\n",
    "# command: python -m nltk.downloader\n",
    "#   in the console\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to construct n-grams of any length n\n",
    "# based on the solution found on\n",
    "# http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "def find_ngrams(input_list, n):\n",
    "    return [str(x[0] + \" \" + x[1]) for x in list(zip(*[input_list[i:] for i in range(n)])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "courses_proc = {}\n",
    "words = {}\n",
    "\n",
    "# construct the dictionaries for the courses and terms\n",
    "for c in courses:\n",
    "    cid = c['courseId']\n",
    "    \n",
    "    # transfer all to lowercase, remove punctuation and numbers\n",
    "    desc = c['description'].lower().translate(str.maketrans('', '', punctuation))\n",
    "    # remove stopwords\n",
    "    desc_proc = [lemmatizer.lemmatize(word, pos='v') for word in desc.split() if word not in stopwords]\n",
    "    \n",
    "    desc_2grams = find_ngrams(desc_proc, 2)\n",
    "    desc_proc.extend(desc_2grams)\n",
    "    \n",
    "    # create a dict of all words\n",
    "    for w in desc_proc:\n",
    "        if w in words:\n",
    "            words[w] += 1\n",
    "        else:\n",
    "            words[w] = 1\n",
    "    \n",
    "    if cid in courses_proc:\n",
    "        # some courseIds appear multiple times\n",
    "        # for those, we decided to append the desctiptions to each other\n",
    "        courses_proc[cid].extend(desc_proc)\n",
    "    else:\n",
    "        courses_proc[cid] = desc_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_filtered = []\n",
    "\n",
    "# filter most and least frequent words\n",
    "for w in words:\n",
    "    if words[w] > MIN and words[w] < MAX:\n",
    "        words_filtered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "courses_proc2 = {}\n",
    "\n",
    "# remove all the words that have been filtered out from the descriptions\n",
    "for c in courses_proc:\n",
    "    courses_proc2[c] = [word for word in courses_proc[c] if word in words_filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain which kinds of cleaning you implemented and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Make sure every courseId only appears once. If it appears more than once, append the two descriptions\n",
    "* Change all cheacters to lowercase, to avoid the same word being considered different. Additionally, all words in stopwords.pkl are in lowercase, so this ensures all words are removed correctly\n",
    "* Remove punctuation, numbers and other symbols, for the same reason as above. Only the words are important, not percentages or numbers.\n",
    "* Remove stopwords, as these don't carry any information about the content of the document\n",
    "* filter most and least frequent words, as they only mess up the results when trying to find similarities between documents. commomn words will be in almost every document, rare ones in almost none.\n",
    "* Lemmatization, to detect only words with different meaning. Identical words, such as \"is\" and \"are\" will automatically be transformed to \"be\", the same is true with multiples (\"documents\" -> \"document\").\n",
    "* n-grams: 2-shingles, to detect frequent combinations of words. Infrequent shingles are filtered just like infrequent words. No higher degrees for the sake of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the terms in the pre-processed description of the $9^{th}$ class in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract', 'abstract', 'abstract', 'activities', 'activities', 'activities', 'activities attend', 'activities attend', 'activities attend', 'al', 'al', 'al', 'algorithmic', 'algorithmic', 'algorithmic', 'architecture', 'architecture', 'architecture', 'assessment', 'assessment', 'assessment', 'assessment methods', 'assessment methods', 'assessment methods', 'assistants', 'assistants', 'assistants', 'assistants forum', 'assistants forum', 'assistants forum', 'attend', 'attend', 'attend', 'attend lecture', 'attend lecture', 'attend lecture', 'automation', 'automation', 'automation', 'basic', 'basic', 'basic', 'bibliography', 'bibliography', 'bibliography', 'bibliothèque', 'bibliothèque', 'bibliothèque', 'build', 'build', 'build', 'challenge', 'challenge', 'challenge', 'cod', 'cod', 'cod', 'combinational', 'combinational', 'combinational', 'comment', 'comment', 'comment', 'complete', 'complete', 'complete', 'complete exercise', 'complete exercise', 'complete exercise', 'complex', 'complex', 'complex', 'components', 'components', 'components', 'computation', 'computation', 'computation', 'concepts', 'concepts', 'concepts', 'concepts', 'concepts', 'concepts', 'concepts start', 'concepts start', 'concepts start', 'content introduction', 'content introduction', 'content introduction', 'control', 'control', 'control', 'creation', 'creation', 'creation', 'cs', 'cs', 'cs', 'de', 'de', 'de', 'definition', 'definition', 'definition', 'deliver', 'deliver', 'deliver', 'describe', 'describe', 'describe', 'description', 'description', 'description', 'description', 'description', 'description', 'design automation', 'design automation', 'design automation', 'design control', 'design control', 'design control', 'design methodologies', 'design methodologies', 'design methodologies', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'design vhdl', 'develop', 'develop', 'develop', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital', 'digital hardware', 'digital hardware', 'digital hardware', 'digital hardware', 'digital hardware', 'digital hardware', 'digital hardware', 'digital hardware', 'digital hardware', 'digital systems', 'digital systems', 'digital systems', 'digital systems', 'digital systems', 'digital systems', 'ed', 'ed', 'ed', 'eda', 'eda', 'eda', 'eda', 'eda', 'eda', 'eda tool', 'eda tool', 'eda tool', 'eda tool', 'eda tool', 'eda tool', 'ee', 'ee', 'ee', 'ee', 'ee', 'ee', 'efficiency', 'efficiency', 'efficiency', 'efficient', 'efficient', 'efficient', 'electronic', 'electronic', 'electronic', 'electronic', 'electronic', 'electronic', 'elements', 'elements', 'elements', 'elements', 'elements', 'elements', 'embed', 'embed', 'embed', 'embed', 'embed', 'embed', 'embed systems', 'embed systems', 'embed systems', 'embed systems', 'embed systems', 'embed systems', 'en', 'en', 'en', 'en bibliothèque', 'en bibliothèque', 'en bibliothèque', 'end student', 'end student', 'end student', 'environment', 'environment', 'environment', 'environment', 'environment', 'environment', 'environment', 'environment', 'environment', 'essential', 'essential', 'essential', 'examination', 'examination', 'examination', 'examination', 'examination', 'examination', 'examination final', 'examination final', 'examination final', 'exercise', 'exercise', 'exercise', 'exercise', 'exercise', 'exercise', 'exercise', 'exercise', 'exercise', 'exercise expect', 'exercise expect', 'exercise expect', 'expect', 'expect', 'expect', 'expect student', 'expect student', 'expect student', 'feature', 'feature', 'feature', 'feedback', 'feedback', 'feedback', 'final', 'final', 'final', 'final examination', 'final examination', 'final examination', 'formalisms', 'formalisms', 'formalisms', 'forum', 'forum', 'forum', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'functional verification', 'fundamental', 'fundamental', 'fundamental', 'guide', 'guide', 'guide', 'guide', 'guide', 'guide', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware', 'hardware description', 'hardware description', 'hardware description', 'hardware description', 'hardware description', 'hardware description', 'hardware model', 'hardware model', 'hardware model', 'homework', 'homework', 'homework', 'homework exercise', 'homework exercise', 'homework exercise', 'hours', 'hours', 'hours', 'hours assistants', 'hours assistants', 'hours assistants', 'httpmoodleepflchcourseviewphpid', 'httpmoodleepflchcourseviewphpid', 'httpmoodleepflchcourseviewphpid', 'important', 'important', 'important', 'important concepts', 'important concepts', 'important concepts', 'include', 'include', 'include', 'individual', 'individual', 'individual', 'integrate', 'integrate', 'integrate', 'integrate exercise', 'integrate exercise', 'integrate exercise', 'introduce', 'introduce', 'introduce', 'introduction', 'introduction', 'introduction', 'issue', 'issue', 'issue', 'keywords', 'keywords', 'keywords', 'lab', 'lab', 'lab', 'language', 'language', 'language', 'language', 'language', 'language', 'language', 'language', 'language', 'languages', 'languages', 'languages', 'languages', 'languages', 'languages', 'layer', 'layer', 'layer', 'learn outcomes', 'learn outcomes', 'learn outcomes', 'learn prerequisites', 'learn prerequisites', 'learn prerequisites', 'lecture complete', 'lecture complete', 'lecture complete', 'lecture integrate', 'lecture integrate', 'lecture integrate', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'link', 'link', 'link', 'link httpmoodleepflchcourseviewphpid', 'link httpmoodleepflchcourseviewphpid', 'link httpmoodleepflchcourseviewphpid', 'logic', 'logic', 'logic', 'logic systems', 'logic systems', 'logic systems', 'methodologies', 'methodologies', 'methodologies', 'methodologies', 'methodologies', 'methodologies', 'methodology', 'methodology', 'methodology', 'methods homework', 'methods homework', 'methods homework', 'methods lecture', 'methods lecture', 'methods lecture', 'midterm', 'midterm', 'midterm', 'midterm examination', 'midterm examination', 'midterm examination', 'model concepts', 'model concepts', 'model concepts', 'model digital', 'model digital', 'model digital', 'model formalisms', 'model formalisms', 'model formalisms', 'model level', 'model level', 'model level', 'model model', 'model model', 'model model', 'modern', 'modern', 'modern', 'moodle', 'moodle', 'moodle', 'moodle', 'moodle', 'moodle', 'moodle link', 'moodle link', 'moodle link', 'moodle page', 'moodle page', 'moodle page', 'note', 'note', 'note', 'noteshandbook', 'noteshandbook', 'noteshandbook', 'notion', 'notion', 'notion', 'office', 'office', 'office', 'office hours', 'office hours', 'office hours', 'open', 'open', 'open', 'outcomes', 'outcomes', 'outcomes', 'outcomes end', 'outcomes end', 'outcomes end', 'page', 'page', 'page', 'plan', 'plan', 'plan', 'prerequisites', 'prerequisites', 'prerequisites', 'prerequisites require', 'prerequisites require', 'prerequisites require', 'principles', 'principles', 'principles', 'problems', 'problems', 'problems', 'proper', 'proper', 'proper', 'proper', 'proper', 'proper', 'quiz', 'quiz', 'quiz', 'recommend', 'recommend', 'recommend', 'recommend course', 'recommend course', 'recommend course', 'require', 'require', 'require', 'require', 'require', 'require', 'require course', 'require course', 'require course', 'resources', 'resources', 'resources', 'resources bibliography', 'resources bibliography', 'resources bibliography', 'ressources', 'ressources', 'ressources', 'ressources en', 'ressources en', 'ressources en', 'reusable', 'reusable', 'reusable', 'review', 'review', 'review', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'rtl', 'sequential', 'sequential', 'sequential', 'soc', 'soc', 'soc', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'socs', 'source', 'source', 'source', 'springer', 'springer', 'springer', 'springer', 'springer', 'springer', 'start', 'start', 'start', 'stateoftheart', 'stateoftheart', 'stateoftheart', 'student activities', 'student activities', 'student activities', 'student describe', 'student describe', 'student describe', 'supervision', 'supervision', 'supervision', 'supervision office', 'supervision office', 'supervision office', 'synthesis', 'synthesis', 'synthesis', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system', 'system design', 'system design', 'system design', 'system level', 'system level', 'system level', 'system level', 'system level', 'system level', 'system level', 'system level', 'system level', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systemc', 'systems cs', 'systems cs', 'systems cs', 'systems design', 'systems design', 'systems design', 'systems design', 'systems design', 'systems design', 'systemsonchip', 'systemsonchip', 'systemsonchip', 'systemsonchip', 'systemsonchip', 'systemsonchip', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'systemverilog systemc', 'task', 'task', 'task', 'teach', 'teach', 'teach', 'teach methods', 'teach methods', 'teach methods', 'techniquesdevelop', 'techniquesdevelop', 'techniquesdevelop', 'tool', 'tool', 'tool', 'tool', 'tool', 'tool', 'users', 'users', 'users', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'verification', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'vhdl', 'websites', 'websites', 'websites', 'work', 'work', 'work']\n"
     ]
    }
   ],
   "source": [
    "# print the fill description of the 9th course\n",
    "print(sorted(courses_proc2[courses[9]['courseId']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Construct an M×N term-document matrix X, where M is the number of terms and N is the number of documents. The matrix X should be sparse. You are not allowed to use libraries for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct mappings to retrieve a term from its index\n",
    "# as well as an index given a term\n",
    "words_index = {}\n",
    "i = 0\n",
    "words_list = []\n",
    "\n",
    "for w in words_filtered:\n",
    "    words_index[w] = i\n",
    "    i += 1\n",
    "    words_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct mappings to retrieve a courseId from its index\n",
    "# as well as an index given a courseId\n",
    "courses_index = {}\n",
    "i = 0\n",
    "courses_list = []\n",
    "\n",
    "for w in courses_proc2:\n",
    "    courses_index[w] = i\n",
    "    i += 1\n",
    "    courses_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters required for constructing the matrix\n",
    "M = len(words_index)\n",
    "N = len(courses_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# matrix containing how many times each words\n",
    "# appears in each document\n",
    "X = np.ndarray((M, N))\n",
    "\n",
    "for c in courses_proc2:\n",
    "    for t in courses_proc2[c]:\n",
    "        X[words_index[t]][courses_index[c]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the 15 terms in the description of the $9^{th}$ class with the highest TF-IDF scores.\n",
    "\n",
    "The TFIDF matrix is constructed transposed, to allow for easier access to the courses, since these are mainly accessed in these exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct the TF matrix, containing the\n",
    "# normalized entries in X by the maximum of\n",
    "# the respective row\n",
    "TF = np.array([x / x.max() for x in X.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct the IDF array, containing the\n",
    "# frequencies of the words within the corpus\n",
    "IDF = np.ndarray(M)\n",
    "for i in range(len(X)):\n",
    "    count = 0\n",
    "    for j in X[i]:\n",
    "        if j > 0:\n",
    "            count += 1\n",
    "    IDF[i] = np.log2(N / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct the TFIDF matrix from the\n",
    "# TF and IDF matrices\n",
    "TFIDF = np.array([x * IDF for x in TF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select only the row related to the 9th course\n",
    "TFIDF_9 = TFIDF[courses_index[courses[9]['courseId']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve the 15 highest entries in TFIDF_9\n",
    "TFIDF_9_top15 = np.argsort(-TFIDF_9)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 15 TERMS FROM TFIDF\n",
      "  6.03765254148 : verification\n",
      "  3.89523690385 : systemverilog\n",
      "  3.89523690385 : systemc\n",
      "  3.89523690385 : functional verification\n",
      "  3.81500253808 : vhdl\n",
      "  3.26125190356 : rtl\n",
      "  2.59682460257 : systemverilog systemc\n",
      "  2.49469492386 : hardware\n",
      "  2.3301579359 : socs\n",
      "  2.3301579359 : design vhdl\n",
      "  1.87141350903 : functional\n",
      "  1.74761845192 : digital hardware\n",
      "  1.56208089818 : digital\n",
      "  1.54761845192 : system level\n",
      "  1.08708396785 : systemsonchip\n"
     ]
    }
   ],
   "source": [
    "print(\"TOP 15 TERMS FROM TFIDF\")\n",
    "for i in TFIDF_9_top15:\n",
    "    print(\" \", TFIDF_9[i], \":\", words_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain where the difference between the large scores and the small ones comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TF gives the importance if a word in a document, between 0 and 1. 1 is the most frequent word, 0 means they never appear.\n",
    "\n",
    "IDF gives the frequency of a word in the corpus, bigger than 0. 0 means the word is in every description, infinity means it's in none of them.\n",
    "\n",
    "TFIDF then is simply the mutliplication of the two, where a high TFIDF means that word is frequent within the desctiption, but rare within the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Search for \"markov chains\" and \"facebook\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the word \"chains\" has been replaced by \"chain\" by lemmatization\n",
    "markov_chain_index = words_index['markov chain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the term \"facebook\" only appeeared 9 times within only one description, and was therefore filtered eaclier. thus, that term has no score and no similarity can be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'facebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-aea2ef02685d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'facebook'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'facebook'"
     ]
    }
   ],
   "source": [
    "words_index['facebook']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the top five courses together with their similarity score for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cosine similarity function, given\n",
    "# the indeces of two courses\n",
    "def sim(i, j):\n",
    "    di = TFIDF[i]\n",
    "    dj = TFIDF[j]\n",
    "    return np.dot(di, dj) / ( np.linalg.norm(di) * np.linalg.norm(dj) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the scores of all courses for\n",
    "# the term 'markov chain' within the TFIDF matrix\n",
    "markov_chain_score = []\n",
    "\n",
    "for i in TFIDF:\n",
    "    markov_chain_score.append(i[markov_chain_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 5 COURSES FOR 'MARKOV CHAIN'\n",
      "  5.94063200363 : MATH-332\n",
      "  4.62049155838 : COM-516\n",
      "  3.46536866878 : MGT-484\n",
      "  1.26013406138 : FIN-408\n",
      "  1.15512288959 : COM-308\n"
     ]
    }
   ],
   "source": [
    "# find the 5 highest scores in markov_chain_score\n",
    "TFIDF_markov_top5 = np.argsort(-np.array(markov_chain_score))[:5]\n",
    "top_markov_names = []\n",
    "\n",
    "print(\"TOP 5 COURSES FOR 'MARKOV CHAIN'\")\n",
    "for i in TFIDF_markov_top5:\n",
    "    top_markov_names.append([i, courses_list[i]])\n",
    "    print(\" \", markov_chain_score[i], \":\", courses_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILARITIES BETWEEN TOP 5 MARKOV CHAINS COURSES\n",
      "          MATH-332 COM-516 MGT-484 FIN-408 COM-308\n",
      "MATH-332    1.00    0.34    0.33    0.18    0.07\n",
      " COM-516    0.34    1.00    0.32    0.21    0.10\n",
      " MGT-484    0.33    0.32    1.00    0.18    0.08\n",
      " FIN-408    0.18    0.21    0.18    1.00    0.04\n",
      " COM-308    0.07    0.10    0.08    0.04    1.00\n"
     ]
    }
   ],
   "source": [
    "# print the similarity scores between the 5 courses\n",
    "# this results in a symmetric matrix with 1s on the diagonal\n",
    "# as the similarity score of a course with itself is 1.00\n",
    "print(\"SIMILARITIES BETWEEN TOP 5 MARKOV CHAINS COURSES\")\n",
    "print(\"         \", top_markov_names[0][1], top_markov_names[1][1], top_markov_names[2][1],\n",
    "      top_markov_names[3][1], top_markov_names[4][1])\n",
    "for i in top_markov_names:\n",
    "    line = \"\" + i[1].rjust(8)\n",
    "    for j in top_markov_names:\n",
    "        line += \"    \" + str(\"%.2f\" % round(sim(i[0], j[0]), 2))\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you think of the results? Give your intuition on what is happening.\n",
    "\n",
    "Since all we looked for were the descriptions where \"markov chain(s)\" appears the most, we expected to have relatively low similartiy scores. It is likely that courses that deal with markov chains have somewhat similar topics, as it is a mathematical concept at the base, but that is not at all reqired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all necessary dictionaries and arrays to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"data/TFIDF.npy\", TFIDF)\n",
    "np.save(\"data/words.npy\", words_list)\n",
    "np.save(\"data/words_dict.npy\", words_index)\n",
    "np.save(\"data/courses.npy\", courses_list)\n",
    "np.save(\"data/courses_dict.npy\", courses_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
